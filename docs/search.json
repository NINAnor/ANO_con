[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Working with the ANO-moduler database",
    "section": "",
    "text": "Introduction\nThis web site show you how to connect to the internal NINA postgreSQL database containing the spatial data for the ANO-modules. The database is found on the t2lippgsql03 server , and is called ano_moduler. This with admin rights are\nThe database contains the raw data needed to make balanced spatial samples of monitoring localities for ANO-modules.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Working with the ANO-moduler database",
    "section": "",
    "text": "Anders Kolstad (anders.kolstad@nina.no)\nJens Åström (jens.astrom@nina.no)",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "001_connecting.html",
    "href": "001_connecting.html",
    "title": "\n1  Connecting to the database\n",
    "section": "",
    "text": "Fisrt you need to save your personal windows password to your local machine.. Don’t write it in your code. If you’re on linux, you can store it in your personal root folder like this (just change the ‘secretPassword’ to you own):\n\nsystem(\"echo '*:*:*:*:secretPassword' &gt; ~/.pgpass\")\nsystem(\"chmod 0600 ~/.pgpass\")\n\nThen you can connect:\n\ncon &lt;- DBI::dbConnect(drv = RPostgres::Postgres(), host = \"t2lippgsql03\", dbname = \"ano_moduler\")",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Connecting to the database</span>"
    ]
  },
  {
    "objectID": "010_SSB500.html",
    "href": "010_SSB500.html",
    "title": "2  SSB500",
    "section": "",
    "text": "2.1 Setup schema\nSetting up a new schema called ssb_grids. Here we can store SSB500, but also SSB10km which may become relevant.\nnew_schemas &lt;- \"CREATE SCHEMA ssb_grids\"\ndbSendQuery(con, new_schemas)\nWrite queries to grant read only access to all.\npriv &lt;- \"ALTER DEFAULT PRIVILEGES IN SCHEMA ssb_grids GRANT SELECT ON TABLES TO ag_pgsql_ano_moduler_ro\"\n\npriv2 &lt;- \"ALTER DEFAULT PRIVILEGES IN SCHEMA ssb_grids GRANT SELECT ON TABLES TO ag_pgsql_ano_moduler_rw\"\n\npriv3 &lt;- \"ALTER DEFAULT PRIVILEGES IN SCHEMA ssb_grids GRANT SELECT ON TABLES TO ag_pgsql_ano_moduler_admin\"\n\npriv4 &lt;- \"GRANT USAGE ON SCHEMA ssb_grids  TO ag_pgsql_ano_moduler_admin\"\npriv5 &lt;- \"GRANT USAGE ON SCHEMA ssb_grids  TO ag_pgsql_ano_moduler_rw\"\npriv6 &lt;- \"GRANT USAGE ON SCHEMA ssb_grids  TO ag_pgsql_ano_moduler_ro\"\n\ndbSendStatement(con, priv)\ndbSendStatement(con, priv2)\ndbSendStatement(con, priv3)\ndbSendStatement(con, priv4)\ndbSendStatement(con, priv5)\ndbSendStatement(con, priv6)",
    "crumbs": [
      "Write geographic data to the database",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>SSB500</span>"
    ]
  },
  {
    "objectID": "010_SSB500.html#read-data-into-r",
    "href": "010_SSB500.html#read-data-into-r",
    "title": "2  SSB500",
    "section": "\n2.2 Read data into R",
    "text": "2.2 Read data into R\nWe use RStudio as the interface when adding new data to the database. We start by bringing the data into our environment.\nFirst we can get the entire SSB500 dataset.\n\nSSBpath &lt;- \"/data/P-Prosjekter2/412421_okologisk_tilstand_2024/Data/SSB0500M_L/ruter500m_Norge.shp\"\nSSB500 &lt;- read_sf(SSBpath) |&gt;\n  st_transform(25833)\n\nStrip down the number of columns\n\nSSB500 &lt;- SSB500 |&gt;\n  select(ssbid = SSBid) # postgre doesnt like capital letters\n\n# the geometry column needs to be named 'geom'\nst_geometry(SSB500) &lt;- \"geom\"\n\nThis data consists of perfect 500x500 grid cells arranged on rounded coordinates in the UTM sone 33 CRS.",
    "crumbs": [
      "Write geographic data to the database",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>SSB500</span>"
    ]
  },
  {
    "objectID": "010_SSB500.html#define-table-properties",
    "href": "010_SSB500.html#define-table-properties",
    "title": "2  SSB500",
    "section": "\n2.3 Define table properties",
    "text": "2.3 Define table properties\nFirst we define the table properties\n\nq1 &lt;- \"create table ssb_grids.ssb_500 (\nssbid character varying(50) primary key,\ngeom geometry(polygon,25833)\n);\"\n\n# indices makes the database work faster. It should be added to all tables that are looked up frequently\nq2 &lt;- \"create index on ssb_grids.ssb_500 using btree(ssbid);\"\nq3 &lt;- \"create index on ssb_grids.ssb_500 using gist(geom);\"\n\n\n# sending the queries:\ndbSendStatement(con, q1)\ndbSendStatement(con, q2)\ndbSendStatement(con, q3)\n\nWe defined geom to be polygon. Now let’s just check that that is true, and there are no multi-polygons for example.\n\nst_geometry_type(SSB500, by_geometry = F)\n\nYes, they are all polygons.",
    "crumbs": [
      "Write geographic data to the database",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>SSB500</span>"
    ]
  },
  {
    "objectID": "010_SSB500.html#write-to-db",
    "href": "010_SSB500.html#write-to-db",
    "title": "2  SSB500",
    "section": "\n2.4 Write to db",
    "text": "2.4 Write to db\nThen we write data to the ssb_500 table.\n\nwrite_sf(SSB500, dsn = con,\n         layer = Id(schema = \"ssb_grids\", table = \"ssb_500\"), \n         append = T)",
    "crumbs": [
      "Write geographic data to the database",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>SSB500</span>"
    ]
  },
  {
    "objectID": "011_bioclimatic_regions.html",
    "href": "011_bioclimatic_regions.html",
    "title": "\n3  Bioclimatic regions\n",
    "section": "",
    "text": "3.1 Setup schema\nSetting up a new schema called helper_variables. Here we can store helper variables that we use to either stratify or balance our spatial sample.\nnew_schemas &lt;- \"CREATE SCHEMA helper_variables\"\ndbSendQuery(con, new_schemas)\nWrite queries to grant read only access to all.\npriv &lt;- \"ALTER DEFAULT PRIVILEGES IN SCHEMA helper_variables GRANT SELECT ON TABLES TO ag_pgsql_ano_moduler_ro\"\n\npriv2 &lt;- \"ALTER DEFAULT PRIVILEGES IN SCHEMA helper_variables GRANT SELECT ON TABLES TO ag_pgsql_ano_moduler_rw\"\n\npriv3 &lt;- \"ALTER DEFAULT PRIVILEGES IN SCHEMA helper_variables GRANT SELECT ON TABLES TO ag_pgsql_ano_moduler_admin\"\n\npriv4 &lt;- \"GRANT USAGE ON SCHEMA helper_variables  TO ag_pgsql_ano_moduler_admin\"\npriv5 &lt;- \"GRANT USAGE ON SCHEMA helper_variables  TO ag_pgsql_ano_moduler_rw\"\npriv6 &lt;- \"GRANT USAGE ON SCHEMA helper_variables  TO ag_pgsql_ano_moduler_ro\"\n\ndbSendStatement(con, priv)\ndbSendStatement(con, priv2)\ndbSendStatement(con, priv3)\ndbSendStatement(con, priv4)\ndbSendStatement(con, priv5)\ndbSendStatement(con, priv6)",
    "crumbs": [
      "Write geographic data to the database",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bioclimatic regions</span>"
    ]
  },
  {
    "objectID": "011_bioclimatic_regions.html#define-table-properties",
    "href": "011_bioclimatic_regions.html#define-table-properties",
    "title": "\n3  Bioclimatic regions\n",
    "section": "\n3.2 Define table properties",
    "text": "3.2 Define table properties\nFirst we define the table properties\n\nq1 &lt;- \"create table helper_variables.bioclimatic_regions (\nssb1000id character varying(50) primary key,\ngeom geometry(polygon,25833)\n);\"\n\n# indices makes the database work faster. It should be added to all tables that are looked up frequently\nq2 &lt;- \"create index on helper_variables.bioclimatic_regions using btree(ssb1000id);\"\nq3 &lt;- \"create index on helper_variables.bioclimatic_regions using gist(geom);\"\n\n\n# sending the queries:\ndbSendStatement(con, q1)\ndbSendStatement(con, q2)\ndbSendStatement(con, q3)\n\nI set the geom column to be *polygon’, but it’s actually a multipolygon. This could cause errors down the line.\n\nst_geometry_type(BCreg, by_geometry = F)\n\nI therefore ran this code in DBBeaver\n\nALTER TABLE HELPER_VARIABLES.BIOCLIMATIC_REGIONS \nALTER COLUMN geom TYPE geometry(MULTIPOLYGON, 25833)",
    "crumbs": [
      "Write geographic data to the database",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bioclimatic regions</span>"
    ]
  },
  {
    "objectID": "011_bioclimatic_regions.html#write-to-db",
    "href": "011_bioclimatic_regions.html#write-to-db",
    "title": "\n3  Bioclimatic regions\n",
    "section": "\n3.3 Write to db",
    "text": "3.3 Write to db\nThen I will write the file to the database.\n\n# turn back into sf object\nBCreg &lt;- BCreg |&gt;\n  st_as_sf()\n\n# check CRS\nst_crs(BCreg) #32633 (we want 25833 i.e. ETRS89 / UTM zone 33N)\n\n# transform\nBCreg &lt;- BCreg |&gt;\n  st_transform(25833)\n\nBCreg &lt;- BCreg |&gt;\n  select(\n    ssb1000id = SSBID,\n    BCregion,\n    Shape\n  )\n# special code to rename the geometry\nst_geometry(BCreg) &lt;- \"geom\"\n\nwrite_sf(BCreg, dsn = con,\n         layer = Id(schema = \"helper_variables\", table = \"bioclimatic_regions\"), \n         append = T)\n\nI also forgot to add a column, so I add it now:\n\nq4 &lt;- \"ALTER TABLE helper_variables.bioclimatic_regions ADD BCregion character varying(50)\"\ndbSendStatement(con, q4)\n\nAnd then update the table with data as well\n\nwrite_sf(BCreg, dsn = con,\n         layer = Id(schema = \"helper_variables\", table = \"bioclimatic_regions\"), \n         append = F)\n\nI also keep a version on the R: server\n\npath_store &lt;- \"/data/P-Prosjekter2/412421_okologisk_tilstand_2024/Data/bioclimaticRegions.gpkg\"\nwrite_sf(BCreg, dsn = path_store,driver = \"GPKG\")\n\n\nlist.files(\"/home/\")\n\n\n\n\n\nBakkestuen, V., Erikstad, L., and Halvorsen, R. 2008. Step-less models for regional environmental variation in Norway. Journal of Biogeography 35(10): 1906–1922. doi:10.1111/j.1365-2699.2008.01941.x.",
    "crumbs": [
      "Write geographic data to the database",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bioclimatic regions</span>"
    ]
  },
  {
    "objectID": "020_samplingFrames.html",
    "href": "020_samplingFrames.html",
    "title": "\n4  Sampling frames\n",
    "section": "",
    "text": "4.1 Setup schema\nThis part sets up the schema, and is only done ones.\nschema_ur &lt;- \"CREATE SCHEMA sampling_frames\"\ndbSendQuery(con, schema_ur)\nWrite queries to grant read only access to all.\npriv &lt;- \"ALTER DEFAULT PRIVILEGES IN SCHEMA sampling_frames GRANT SELECT ON TABLES TO ag_pgsql_ano_moduler_ro\"\npriv2 &lt;- \"ALTER DEFAULT PRIVILEGES IN SCHEMA sampling_frames GRANT SELECT ON TABLES TO ag_pgsql_ano_moduler_rw\"\npriv3 &lt;- \"ALTER DEFAULT PRIVILEGES IN SCHEMA sampling_frames GRANT SELECT ON TABLES TO ag_pgsql_ano_moduler_admin\"\npriv4 &lt;- \"GRANT USAGE ON SCHEMA sampling_frames  TO ag_pgsql_ano_moduler_admin\"\npriv5 &lt;- \"GRANT USAGE ON SCHEMA sampling_frames  TO ag_pgsql_ano_moduler_rw\"\npriv6 &lt;- \"GRANT USAGE ON SCHEMA sampling_frames  TO ag_pgsql_ano_moduler_ro\"\n\ndbSendStatement(con, priv)\ndbSendStatement(con, priv2)\ndbSendStatement(con, priv3)\ndbSendStatement(con, priv4)\ndbSendStatement(con, priv5)\ndbSendStatement(con, priv6)",
    "crumbs": [
      "Write tabular data to the database",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Sampling frames</span>"
    ]
  },
  {
    "objectID": "020_samplingFrames.html#ano-våtmark",
    "href": "020_samplingFrames.html#ano-våtmark",
    "title": "\n4  Sampling frames\n",
    "section": "\n4.2 ANO våtmark",
    "text": "4.2 ANO våtmark\nHere we upload the dataframe with SSB-id’s that make up the sampling frame for ANO våtmark (wetlands). The dataframe also contain auxillary variables used for balansing and stratifying the sampling. All the datasets and code (python) to create this dataframe if or will be uploaded to the database as well.\nRead data into R:\n\nlibrary(readr)\ndat &lt;- read_delim(\"/data/P-Prosjekter2/412421_okologisk_tilstand_2024/Jan/ruter500m_Norge.csv\", \n    delim = \";\", escape_double = FALSE,  trim_ws = TRUE)\n\n# removing some columns\ndat &lt;- dat |&gt;\n  select(-Join_Count,\n         -TARGET_FID,\n         -Shape_Leng,\n         -Shape_Length,\n         -Shape_Area)\n\n# There are some columns that should be numeric, but are read initial as characters with leading apostrophes and the wrong decimal sign.\ntoConvert &lt;- c(\n  \"rFerskvann\",\n  \"rHav\",\n  \"rSnoIsbre\",\n  \"rBebygdSamf\",\n  \"minSlope\",\n  \"meanSlope\",\n  \"maxSlope\"\n)\n\ndat &lt;- dat |&gt;\n  mutate(across(toConvert, ~ gsub(\"^'\", \"\", .))) |&gt;\n  mutate(across(toConvert, ~ gsub(\",\", \".\", .))) |&gt;\n  mutate(across(toConvert, as.numeric)) |&gt;\n  mutate(across(toConvert, \\(x) round(x, digits = 5))) |&gt;\n  rename(ssbid = SSBid)\n\nWe start with 1 880 382 sampling units, but we want to exclude sampling units with 100% water (ocean or freshwater). There are 516201 units of marine waters and 13263 with freshwater. We could perhaps also exclude units with 100% build up area, but I’m at this moment not sure we can be fully sure there is no wetland in those units. Units with snow and ice are kept, becaus ethey can become wetalnds in a few decades.\n\ndat_full &lt;- dat\n\nnrow(dat_full[dat_full$rHav == 100,])\nnrow(dat_full[dat_full$rFerskvann == 100,])\n  \ndat &lt;- dat |&gt;\n  dplyr::filter(rHav &lt; 100 & rFerskvann &lt;100)\n\nnrow(dat_full)-nrow(dat) # 529464 Correct\nnrow(dat) #1350918\nnrow(dat)/4 # 337 729.5\n\n\nanyDuplicated(dat$ssbid) # no duplicates\n\nNow we are down to 1 644 092 units. If we divide by 4 to get km2, we get a little higher than the true value for the Norwegian mainland, which is as expected since we include some coastal waters.\nWe also need a primary key, and therefore I add a unique identifier.\n\nids &lt;- uuid::UUIDgenerate(n = nrow(dat))\nanyDuplicated(ids)\ndat &lt;- dat |&gt;\n  mutate(\n    vaatmark_id = ids\n  )\n\n\ndat &lt;- dat |&gt;\n  select(\n    ssbid,\n    vaatmark_id,\n    ost,\n    nord,\n    centroid_x = CENTROID_X,\n    centroid_y = CENTROID_Y,\n    komm2016 = Komm2016,\n    fylk2016 = Fylk2016,\n    r_ferskvann = rFerskvann,\n    r_hav = rHav,\n    r_sno_isbre = rSnoIsbre,\n    r_bebygd_samf = rBebygdSamf,\n    mean_slope = meanSlope,\n    ssbid_1000 = SSBid_1000,\n    sone_kode = Sone_kode,\n    seksjon_kode = Seksjon_kode,\n    abi = ABI)\n\n\n4.2.1 Define table properties\nWe will name the table samplingframe_vaatmark_2025.\n\nq1 &lt;- \"create table sampling_frames.samplingframe_vaatmark_2025 (\nvaatmark_id character varying(50) primary key,\nssbid character varying(50),\nost integer,\nnord integer,\ncentroid_x numeric(24,0),\ncentroid_y numeric(24,0),\nkomm2016 character varying(5),\nfylk2016 character varying(5),\nr_ferskvann numeric(10,5),\nr_hav numeric(10,5),\nr_sno_isbre numeric(10,5),\nr_bebygd_samf numeric(10,5),\nmean_slope numeric(10,5),\nssbid_1000 numeric(15,0),\nsone_kode character varying(7),\nseksjon_kode character varying(7),\nabi integer,\nCONSTRAINT fk_ssb_500\n      FOREIGN KEY (ssbid)\n        REFERENCES ssb_grids.ssb_500 (ssbid)\n);\"\n\n# indices makes the database work faster. It should be added to all tables that are looked up frequently\nq2 &lt;- \"create index on sampling_frames.samplingframe_vaatmark_2025 using btree(ssbid);\"\nq3 &lt;- \"create index on sampling_frames.samplingframe_vaatmark_2025 using btree(vaatmark_id);\"\n\n\n# sending the queries:\ndbSendStatement(con, q1)\ndbSendStatement(con, q2)\ndbSendStatement(con, q3)\n\n\n4.2.2 Write to db\nThen we write data to the samplingframe_havstrand_2024 table.\n\nwrite_sf(dat, dsn = con,\n         layer = Id(schema = \"sampling_frames\", table = \"samplingframe_vaatmark_2025\"), \n         append = T)",
    "crumbs": [
      "Write tabular data to the database",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Sampling frames</span>"
    ]
  },
  {
    "objectID": "020_samplingFrames.html#ano-havstrand",
    "href": "020_samplingFrames.html#ano-havstrand",
    "title": "\n4  Sampling frames\n",
    "section": "\n4.3 ANO Havstrand",
    "text": "4.3 ANO Havstrand\nFor ANO Havstrand (ANO Coast), SSB500 has been masked to only include grid cells that overlap with the Norwegian coastline. The script for doing that is written in python ad can be found here: /data/P-Prosjekter2/412421_okologisk_tilstand_2024/Jan/\nRead data into R:\n\ncoast_path &lt;- \"/data/P-Prosjekter2/412421_okologisk_tilstand_2024/Jan/CoastalSampling3.gdb\"\n\nsf::st_layers(coast_path)\n\nDriver: OpenFileGDB \nAvailable layers:\n                layer_name     geometry_type features fields\n1            fcNorgeKyst_l Multi Line String   101331      2\n2 fcKystline_500mSegmenter Multi Line String   240861      1\n3              Samples62_5       Multi Point   769342      2\n4              Samples83_3       Multi Point   555958      2\n5               Samples125       Multi Point   350676      2\n6            SSB500_Coast4     Multi Polygon   121920      6\n7        Hexagon62_5_Coast     Multi Polygon  2729978      3\n8        Hexagon83_3_Coast     Multi Polygon  1622139      3\n9         Hexagon125_Coast     Multi Polygon   796190      3\n               crs_name\n1 ETRS89 / UTM zone 33N\n2 ETRS89 / UTM zone 33N\n3 ETRS89 / UTM zone 33N\n4 ETRS89 / UTM zone 33N\n5 ETRS89 / UTM zone 33N\n6 ETRS89 / UTM zone 33N\n7 WGS 84 / UTM zone 33N\n8 WGS 84 / UTM zone 33N\n9 WGS 84 / UTM zone 33N\n\n\nSSB500_Coast4 contains the SSB500 grid cells that overlap with the coastline.\n\ncoastSSB &lt;- read_sf(coast_path, layer = \"SSB500_Coast4\")\n\nThe SSBid will be the foreign key, linking to ssb_500. We also need a primary key, and therefore I add a unique identifier.\n\nids &lt;- UUIDgenerate(n = nrow(coastSSB))\nanyDuplicated(ids)\ncoastSSB &lt;- coastSSB |&gt;\n  select(ssbid = SSBid)|&gt;\n  mutate(havstrand_id = ids)\n\nWe don’t need the geometry\n\ncoastSSB &lt;- as_tibble(coastSSB) |&gt;\n  select(ssbid, havstrand_id)\n\n\n4.3.1 Define table properties\nWe will name the table samplingframe_havstrand_2024.\n\nq1 &lt;- \"create table sampling_frames.samplingframe_havstrand_2024 (\nhavstrand_id character varying(50) primary key,\nssbid character varying(50),\nCONSTRAINT fk_ssb_500\n      FOREIGN KEY (ssbid)\n        REFERENCES ssb_grids.ssb_500 (ssbid)\n);\"\n\n# indices makes the database work faster. It should be added to all tables that are looked up frequently\nq2 &lt;- \"create index on sampling_frames.samplingframe_havstrand_2024 using btree(ssbid);\"\nq3 &lt;- \"create index on sampling_frames.samplingframe_havstrand_2024 using btree(havstrand_id);\"\n\n\n# sending the queries:\ndbSendStatement(con, q1)\ndbSendStatement(con, q2)\ndbSendStatement(con, q3)\n\n\n4.3.2 Write to db\nThen we write data to the samplingframe_havstrand_2024 table.\n\nwrite_sf(coastSSB, dsn = con,\n         layer = Id(schema = \"sampling_frames\", table = \"samplingframe_havstrand_2024\"), \n         append = T)",
    "crumbs": [
      "Write tabular data to the database",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Sampling frames</span>"
    ]
  },
  {
    "objectID": "021_readFromDb.html",
    "href": "021_readFromDb.html",
    "title": "\n5  Read data from the database\n",
    "section": "",
    "text": "5.1 Overview\nNow that we have added data to the database, we can also read it back.\ncoast_sample &lt;- dplyr::tbl(con, dbplyr::in_schema(\"sampling_frames\", \"samplingframe_havstrand_2024\"))\ncoast_sample\n\n# Source:   SQL [?? x 2]\n# Database: postgres  [anders.kolstad@t2lippgsql03:5432/ano_moduler]\n   havstrand_id                         ssbid         \n   &lt;chr&gt;                                &lt;chr&gt;         \n 1 4de8478a-d2c8-4730-83dd-12ac25d4fa07 22495006594500\n 2 bc9a80d7-c434-47e9-a89a-7f7a3f59f625 22495006595000\n 3 ce7166b6-cb49-499a-a99f-9da0e274abd8 22495006595500\n 4 e56036c5-7fe4-461a-a022-44367119acac 22495006596000\n 5 1f592dac-2fb7-49e5-9aa9-fc0cf9e9184f 22490006596500\n 6 20394fe8-305c-4c39-add9-f013295ca932 22495006596500\n 7 3d3ffe94-0400-4c1c-9a0f-62f4d6c29c4d 22490006597000\n 8 4d199401-dd20-48ef-b6c6-8de00b9eb9a4 22495006597000\n 9 a34df6f4-8f92-4976-b7ae-94b176fa73b1 22625006542000\n10 89e8296e-e076-4519-b916-60c5ef922292 22625006542500\n# ℹ more rows\nSame for the wetlands samples:\nvaatmark_2025 &lt;- dplyr::tbl(con, dbplyr::in_schema(\"samples\", \"vaatmark_2025\"))",
    "crumbs": [
      "Read data from database",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Read data from the database</span>"
    ]
  },
  {
    "objectID": "030_simulation.html",
    "href": "030_simulation.html",
    "title": "\n6  Simulation\n",
    "section": "",
    "text": "6.1 Create dummy dataset\nI made an R package spaceR to house the main function for the ANO sampling. In that package I also added a dummy dataset.\n#library(spaceR)\ndata('mypop')\nDT::datatable(mypop)\n\n\nTable 6.1: Dummy data\nThe probabilities all sum to the target n.\nmypop |&gt;\n  group_by(stratum) |&gt;\n  summarise(sum_pi_h = sum(pi_h),\n            sum_pi_area = sum(pi_area),\n            sum_area = sum(area))\nmypop |&gt;\n  pivot_longer(cols = starts_with(c(\"res\", \"aux\"))) |&gt;\n  ggplot() +\n  geom_histogram(aes(value),\n    bins = 50\n  ) +\n  facet_grid(\n    rows = vars(stratum), cols = vars(name),\n    scales = \"free\"\n  ) +\n  theme_bw()\n\n\n\n\n\n\nFigure 6.1: Histogram of response and auxilary variables in the dummy dataset.\ntableOfMeans &lt;- mypop |&gt;\n  pivot_longer(\n    cols = starts_with(c(\"res\", \"aux\", \"North\", \"East\")),\n    names_to = \"variable\", values_to = \"values\"\n  ) |&gt;\n  group_by(variable) |&gt;\n  summarise(\"Population mean\" = round(mean(values), 2),\n            \"Population variance\" = round(var(values), 2)) |&gt;\n  pivot_longer(cols = ends_with(\"mean\")) |&gt;\n  select(Variable = variable, \n         Sample = name,\n         mean = value, \n         variance = \"Population variance\") |&gt;\n  mutate(Sample = \"Population\")\n\ntableOfMeans |&gt;\n  kable()\n\n\nTable 6.2: Population means. The aux variables are not correlated to the strata, but the res (response) variables do differ between strata.\n\n\n\n\nVariable\nSample\nmean\nvariance\n\n\n\nEasting\nPopulation\n30.00\n300.00\n\n\nNorthing\nPopulation\n30.00\n300.00\n\n\naux1\nPopulation\n11.09\n78.68\n\n\naux2\nPopulation\n0.41\n0.41\n\n\nres1\nPopulation\n13.98\n30.11\n\n\nres2\nPopulation\n27.23\n198.02\nmypop |&gt;\n  pivot_longer(\n    cols = starts_with(c(\"res\", \"aux\")),\n    names_to = \"variable\", values_to = \"values\"\n  ) |&gt;\n  group_by(variable, stratum) |&gt;\n  summarise(mean = round(mean(values), 2)) |&gt;\n  kable()\n\n`summarise()` has grouped output by 'variable'. You can override using the\n`.groups` argument.\n\n\n\nTable 6.3: Means for each strata. The aux variables are not correlated to the strata, but the res (response) variables do differ between strata.\n\n\n\n\nvariable\nstratum\nmean\n\n\n\naux1\nA\n11.09\n\n\naux1\nB\n11.09\n\n\naux1\nC\n11.09\n\n\naux1\nD\n11.09\n\n\naux2\nA\n0.42\n\n\naux2\nB\n0.44\n\n\naux2\nC\n0.40\n\n\naux2\nD\n0.42\n\n\nres1\nA\n4.95\n\n\nres1\nB\n10.03\n\n\nres1\nC\n14.96\n\n\nres1\nD\n19.99\n\n\nres2\nA\n50.02\n\n\nres2\nB\n9.98\n\n\nres2\nC\n30.06\n\n\nres2\nD\n14.01\nlw &lt;- .1\nggpubr::ggarrange(\n  ggplot(data = mypop) +\n    geom_tile(mapping = aes(x = Easting, y = Northing, fill = factor(stratum)), \n              width = 1, height = 1, linewidth = lw, colour = \"white\") +\n    scale_fill_viridis_d(name = \"Stratum\") +\n    coord_fixed(),\n  \n  ggplot(data = mypop) +\n    geom_tile(mapping = aes(x = Easting, y = Northing, fill = aux1), \n              width = 1, height = 1, linewidth = lw, colour = \"white\") +\n    scale_fill_viridis_c(name = \"aux1\") +\n    coord_fixed(),\n  \n  ggplot(data = mypop) +\n    geom_tile(mapping = aes(x = Easting, y = Northing, fill = aux2), \n              width = 1, height = 1, linewidth = lw, colour = \"white\") +\n    scale_fill_viridis_c(name = \"aux2\") +\n    coord_fixed(),\n  \n  ggplot(data = mypop) +\n    geom_tile(mapping = aes(x = Easting, y = Northing, fill = res1), \n              width = 1, height = 1, linewidth = lw, colour = \"white\") +\n    scale_fill_viridis_c(name = \"res1\") +\n    coord_fixed(),\n  \n  \n  ggplot(data = mypop) +\n    geom_tile(mapping = aes(x = Easting, y = Northing, fill = res2), \n              width = 1, height = 1, linewidth = lw, colour = \"white\") +\n    scale_fill_viridis_c(name = \"res2\") +\n    coord_fixed(),\n  \n  ggplot(data = mypop) +\n    geom_tile(mapping = aes(x = Easting, y = Northing, fill = factor(area)), \n              width = 1, height = 1, linewidth = lw, colour = \"white\") +\n    scale_fill_viridis_d(name = \"area\") +\n    coord_fixed()\n)\n\n\n\n\n\n\nFigure 6.2: Spatial distribution of the strata, and the four variables in the dummy dataset.",
    "crumbs": [
      "Spatial sampling",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Simulation</span>"
    ]
  },
  {
    "objectID": "030_simulation.html#stratified-simple-random-sample-ssrs",
    "href": "030_simulation.html#stratified-simple-random-sample-ssrs",
    "title": "\n6  Simulation\n",
    "section": "\n6.2 Stratified simple random sample (SSRS)",
    "text": "6.2 Stratified simple random sample (SSRS)\nThen I use the cube method in to select a sample balanced on the strata, essentially creating a stratified simple random sample. Here I use the sampling package. This is just a simple first test.\n\n# Create model matrix by expanding the strata column, explicitly assigning both presences and absences to each row.\nX &lt;- model.matrix(~ stratum -1, mypop)\nDT::datatable(X)\n\n\nTable 6.4: Model matrix\n\n\n\n\n\n\n\n\n\n\nset.seed(314)\nsample_ind &lt;- sampling::samplecube(X = X, pik = mypop$pi_h, comment = TRUE, method = 1)\n\n\nBEGINNING OF THE FLIGHT PHASE\nThe matrix of balanced variable has 4  variables and  3600  units\nThe size of the inclusion probability vector is  3600 \nThe sum of the inclusion probability vector is  20 \nThe inclusion probability vector has  3600  non-integer elements\nStep 1  Step  2 ,  \n\nNO LANDING PHASE\n\nQUALITY OF BALANCING\n         TOTALS HorvitzThompson_estimators Relative_deviation\nstratumA    720                        720      -2.526374e-13\nstratumB    360                        360      -1.578984e-13\nstratumC   1440                       1440      -2.842171e-13\nstratumD   1080                       1080       3.157968e-13\n\neps &lt;- 1e-6\nmysample &lt;- mypop[sample_ind &gt; (1 - eps), ]\n\nggplot(data = mypop) +\n  geom_tile(mapping = aes(x = Easting, y = Northing, fill = factor(stratum)), width = 1, height = 1, size = 0.5, colour = \"white\") +\n  geom_tile(data = mysample, mapping = aes(x = Easting, y = Northing), fill = NA, width = 1, height = 1, linewidth = 0.7, colour = \"black\") +\n  scale_fill_viridis_d(name = \"Stratum\") +\n  coord_fixed()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\nFigure 6.3: Sample balanced on a categorical variable (strata). This is equivalent to a stratified simple random sample.\n\n\n\n\nIn Figure 6.3, n= 5 for each strata. This is defined by the the sum of the inclusion probability vector, which is 20 in total.",
    "crumbs": [
      "Spatial sampling",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Simulation</span>"
    ]
  },
  {
    "objectID": "030_simulation.html#balanced-sample",
    "href": "030_simulation.html#balanced-sample",
    "title": "\n6  Simulation\n",
    "section": "\n6.3 Balanced sample",
    "text": "6.3 Balanced sample\nNow I want to balance the sample on two auxiliary variables, as well as on the strata.\n\n# Create model matrix \nX2 &lt;- model.matrix(~ stratum + aux1 + aux2 - 1, mypop)\n#DT::datatable(X2)\n\n\nset.seed(3141)\nsample_ind2 &lt;- sampling::samplecube(X = X2, pik = mypop$pi_h, comment = TRUE, method = 1)\n\n\nBEGINNING OF THE FLIGHT PHASE\nThe matrix of balanced variable has 6  variables and  3600  units\nThe size of the inclusion probability vector is  3600 \nThe sum of the inclusion probability vector is  20 \nThe inclusion probability vector has  3600  non-integer elements\nStep 1  Step  2 ,  \n\n\nBEGINNING OF THE LANDING PHASE\nAt the end of the flight phase, there remain  4 non integer probabilities \nThe sum of these probabilities is  2 \nThis sum is  integer\nThe linear program will consider  6  possible samples\nThe mean cost is  0.001549196 \nThe smallest cost is  0.0002937136 \nThe largest cost is  0.003884496 \nThe cost of the selected sample is 0.0005104717\n\nQUALITY OF BALANCING\n           TOTALS HorvitzThompson_estimators Relative_deviation\nstratumA   720.00                    720.000       0.000000e+00\nstratumB   360.00                    360.000       2.210577e-13\nstratumC  1440.00                   1440.000      -9.473903e-14\nstratumD  1080.00                   1080.000       1.052656e-13\naux1     39916.68                  40781.592       2.166788e+00\naux2      1490.67                   1308.384      -1.222846e+01\n\nmysample2 &lt;- mypop[sample_ind2 &gt; (1 - eps), ]\n\nggplot(data = mypop) +\n  geom_tile(mapping = aes(x = Easting, y = Northing, fill = factor(stratum)), width = 1, height = 1, size = 0.5, colour = \"white\") +\n  geom_tile(data = mysample2, mapping = aes(x = Easting, y = Northing), fill = NA, width = 1, height = 1, linewidth = 0.7, colour = \"black\") +\n  scale_fill_viridis_d(name = \"Stratum\") +\n  coord_fixed()\n\n\n\n\n\n\nFigure 6.4: Sample balanced on two auxilary variables, as well as the categorical strata.",
    "crumbs": [
      "Spatial sampling",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Simulation</span>"
    ]
  },
  {
    "objectID": "030_simulation.html#balanced-and-well-spread",
    "href": "030_simulation.html#balanced-and-well-spread",
    "title": "\n6  Simulation\n",
    "section": "\n6.4 Balanced and well-spread",
    "text": "6.4 Balanced and well-spread\nThe samples do look like they are well-spread already, and not clumped. This might get different if n was bigger. Figure 6.5 shows how two and three neighboring cells frequently get selected.\n\nmypop &lt;- mypop |&gt;\n  mutate(pi_h2 = rep(40, 4) / N_h)\n\nset.seed(3143)\nsample_ind5 &lt;- sampling::samplecube(X = X2, pik = mypop$pi_h2, comment = TRUE, method = 1)\n\n\nBEGINNING OF THE FLIGHT PHASE\nThe matrix of balanced variable has 6  variables and  3600  units\nThe size of the inclusion probability vector is  3600 \nThe sum of the inclusion probability vector is  160 \nThe inclusion probability vector has  3600  non-integer elements\nStep 1  Step  2 ,  \n\n\nBEGINNING OF THE LANDING PHASE\nAt the end of the flight phase, there remain  4 non integer probabilities \nThe sum of these probabilities is  2 \nThis sum is  integer\nThe linear program will consider  6  possible samples\nThe mean cost is  0.00200005 \nThe smallest cost is  0.0001918533 \nThe largest cost is  0.002723267 \nThe cost of the selected sample is 0.0001918533\n\nQUALITY OF BALANCING\n           TOTALS HorvitzThompson_estimators Relative_deviation\nstratumA   720.00                    720.000       0.000000e+00\nstratumB   360.00                    360.000      -4.736952e-14\nstratumC  1440.00                   1440.000       0.000000e+00\nstratumD  1080.00                   1080.000      -2.105312e-14\naux1     39916.68                  39897.576      -4.786470e-02\naux2      1490.67                   1475.802      -9.974039e-01\n\nmysample5 &lt;- mypop[sample_ind5 &gt; (1 - eps), ]\n\nggplot(data = mypop) +\n  geom_tile(mapping = aes(x = Easting, y = Northing, fill = factor(stratum)), width = 1, height = 1, size = 0.5, colour = \"white\") +\n  geom_tile(data = mysample5, mapping = aes(x = Easting, y = Northing), fill = NA, width = 1, height = 1, linewidth = 0.7, colour = \"black\") +\n  scale_fill_viridis_d(name = \"Stratum\") +\n  coord_fixed()\n\n#ggsave(\"img/balanced.png\")\n\n\n\n\n\n\nFigure 6.5: Balanced sample with n = 40\n\n\n\n\nWe can spread the samples geographically by balancing on the coordinates, but there are better ways, such as the local pivotal method. This is referred to as doubly balanced sampling. In Figure 6.6, sampling units are more spread, but can still occur next to each other.\n\n# model matrix, without the strata, which we now can add as a vector on the side\nX4 &lt;- model.matrix(~ aux1 + aux2 - 1, mypop)\n\n# define spreading variables (can be UTM coordinates)\nXspread &lt;- cbind(mypop$Easting, mypop$Northing)\nset.seed(314)\n\n\nsample_ind6 &lt;- BalancedSampling::lcubestratified(\n  prob = mypop$pi_h2,\n  Xspread = Xspread,\n  Xbal = X4,\n  integerStrat = mypop$stratum)\nmysample6 &lt;- mypop[sample_ind6,]\n\nggplot(data = mypop) +\n  geom_tile(mapping = aes(x = Easting, y = Northing, fill = factor(stratum)), width = 1, height = 1, size = 0.5, colour = \"white\") +\n  geom_tile(data = mysample6, mapping = aes(x = Easting, y = Northing), fill = NA, width = 1, height = 1, linewidth = 0.7, colour = \"black\") +\n  scale_fill_viridis_d(name = \"Stratum\") +\n  coord_fixed()\n#ggsave(\"img/balanced_well_spread.png\")\n\n\n\n\n\n\nFigure 6.6: Balanced and well-spread sample (n per strata is 40).",
    "crumbs": [
      "Spatial sampling",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Simulation</span>"
    ]
  },
  {
    "objectID": "030_simulation.html#pps",
    "href": "030_simulation.html#pps",
    "title": "\n6  Simulation\n",
    "section": "\n6.5 PPS",
    "text": "6.5 PPS\nNext i want to introduce sampling with ppr. The purpose is that we don’t want to spend too much time going to remote places only to find that the sampling unit is mostly just water. We want to reduce the change of selecting those units, but still keep the probability above 0 so that we can use the HT-estimator and get area-representative estimates of the population.\nPPR can be rather complex, and I’m not sure the solution here is the best or most correct. Basically, rather than calculating the inclusion probability as \\(n/N\\), I use \\(area/sum(area)\\).\n\nset.seed(3143)\nsample_ind4 &lt;- BalancedSampling::lcubestratified(\n  prob = mypop$pi_area,\n  Xspread = Xspread,\n  Xbal = X4,\n  integerStrat = mypop$stratum)\nmysample4 &lt;- mypop[sample_ind4, ]\n\nIn our example, using pps disfavored population units below a Northing of 30 (Figure 6.7), but kept the n as before.\n\nggplot(data = mypop) +\n  geom_tile(mapping = aes(x = Easting, y = Northing, fill = factor(stratum)), width = 1, height = 1, size = 0.5, colour = \"white\") +\n  geom_tile(data = mysample4, mapping = aes(x = Easting, y = Northing), fill = NA, width = 1, height = 1, linewidth = 0.7, colour = \"black\") +\n  scale_fill_viridis_d(name = \"Stratum\") +\n  coord_fixed()\n\n\n\n\n\n\nFigure 6.7: Balanced sample with inclusion probability proportional to size (area).",
    "crumbs": [
      "Spatial sampling",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Simulation</span>"
    ]
  },
  {
    "objectID": "030_simulation.html#comparison",
    "href": "030_simulation.html#comparison",
    "title": "\n6  Simulation\n",
    "section": "\n6.6 Comparison",
    "text": "6.6 Comparison\n\nvoronoi_ssrs &lt;- BalancedSampling::sb(mypop$pi_h, X2, which(sample_ind&gt;(1-eps)))\nvoronoi_bal &lt;- BalancedSampling::sb(mypop$pi_h, X2, which(sample_ind2&gt;(1-eps)))\nvoronoi_bal2 &lt;- BalancedSampling::sb(mypop$pi_h, X2, which(sample_ind5&gt;(1-eps)))\nvoronoi_bal_spred &lt;- BalancedSampling::sb(mypop$pi_h, X2, which(sample_ind6&gt;(1-eps)))\n\nThe voronoi measure of spatial balance, as a ratio against SSRS, is:\n\nBalanced sample: 2.5899316\nBalanced sample with large n: 2.3893389\nBalanced and well-spread: 6.0384541\n\nIn other words, the balanced and well spread is considerably better than just balanced, and all balanced samples re considerably better than simple random samples. We did not includ pps in this comparison.\nThe relative differences between the Horvitz-Thompson estimators for the population means is very similar across all our samples. Aux2 is providing the best test of balance perhaps, because it is an exponential variable. The balanced and well-spread sample had the smallest relative deviation for thet variable. But it could also be that n=5 is too little for getting stable and robust measures of deviation. A better validation would be to bootstrap the process and look at the variation in means then.\n\nht_results &lt;- lapply(vars, function(v) {\n  y &lt;- mysample6[[v]]\n  res &lt;- mase::horvitzThompson(y = y, pi = mysample6$pi_area, var_est = TRUE)\n  data.frame(\n    Balanced_spread = round(res$pop_mean, 2),\n    Balanced_spread_var = round(res$pop_mean_var,2)\n  )\n}) %&gt;%\n  bind_rows()\n\ntableOfMeans &lt;- tableOfMeans |&gt;\n  cbind(ht_results) |&gt;\n  mutate(Balanced_spread = mean/Balanced_spread)\n\ntableOfMeans |&gt;\n  select(!ends_with(\"var\"),\n         -mean, -variance, -Sample) |&gt;\n  pivot_longer(cols = !Variable) |&gt;\n  pivot_wider(names_from = Variable, values_from = value) |&gt;\n  kable()\n\n\nTable 6.5: Relative differences between HT-estimators for the population mean\n\n\n\n\n\n\n\n\n\n\n\n\n\nname\nEasting\nNorthing\naux1\naux2\nres1\nres2\n\n\n\nSRSS\n0.9759271\n1.1718750\n1.2028200\n1.782609\n1.0130435\n1.0036860\n\n\nBalanced\n1.0366275\n1.0452962\n0.9788173\n1.138889\n0.9872881\n0.9507682\n\n\nPPS\n0.9299442\n0.7915567\n0.7427997\n0.820000\n0.9568789\n1.0878945\n\n\nBalanced_spread\n1.0211028\n1.3599275\n1.5730496\n1.078947\n1.0000000\n1.0229151",
    "crumbs": [
      "Spatial sampling",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Simulation</span>"
    ]
  },
  {
    "objectID": "030_simulation.html#hierarchical-sampling",
    "href": "030_simulation.html#hierarchical-sampling",
    "title": "\n6  Simulation\n",
    "section": "\n6.7 Hierarchical sampling",
    "text": "6.7 Hierarchical sampling\nI want to create hierarchical samples that get reduced sequentially by x number of units each time. I will use x = 20 here to keep things simple. I can create a new sample based on the previous sample, but one worry is that any differences between the initial sample and the population can get reproduced, causing a kind of drift in the auxilary variables.\nOne option is then to sample, not from the consecutive sample, but from the entire population, and instead set the probabilities (pi) for those that were not already selected, to zero. That way the sub-samples are still balanced relative to the entire population.\nThe spatial spreading, however, is best to recalculate for each subsequent sample. Therefore we calculate Xspread again each time, but offsetting he coordinates of those population units with probabilities = 0.\n\nh_samples &lt;- nested_balanced(mypop,\n  n_seq = seq(100, 20, -20),\n  id_col = \"myids\",\n  stratum_col = \"stratum\",\n  easting_col = \"Easting\",\n  northing_col = \"Northing\",\n  area_col = \"area2\",\n  xbal_formula = ~aux1+aux2-1,\n  exclude_offset = 1e+06,\n  return_dataframe = TRUE,\n  quiet = TRUE)\n\n\nn100 &lt;- mypop |&gt;\n  filter(ID %in% h_samples$mysample_100$ID)\nn20 &lt;- mypop |&gt;\n  filter(ID %in% h_samples$mysample_20$ID)\n\nggplot(data = mypop) +\n  geom_tile(\n    mapping = aes(x = Easting, y = Northing, fill = factor(stratum)),\n    width = 1, height = 1, size = 0.5, colour = \"white\"\n  ) +\n  geom_tile(\n    data = n100, mapping = aes(x = Easting, y = Northing),\n    fill = NA, width = 1, height = 1, linewidth = 0.7, colour = \"black\"\n  ) +\n  geom_tile(\n    data = n20, mapping = aes(x = Easting, y = Northing),\n    fill = \"white\", width = 1, height = 1, linewidth = 0.7, colour = \"black\"\n  ) +\n  scale_fill_viridis_d(name = \"Stratum\") +\n  coord_fixed()\n\n\n\n\n\n\n\n\nall_selected &lt;- c(\n  h_samples$mysample_100$ID,\n  h_samples$mysample_80$ID,\n  h_samples$mysample_60$ID,\n  h_samples$mysample_40$ID,\n  h_samples$mysample_20$ID\n)\n\n\n all_selected |&gt;\n  as_tibble() |&gt;\n  rename(ID = value) |&gt;\n  group_by(ID) |&gt;\n  summarise(no_selected = n()) |&gt;\n  right_join(mypop, by = \"ID\") |&gt;\n  ggplot() +\n  geom_tile(mapping = aes(x = Easting, y = Northing, fill = factor(no_selected)), \n            width = 1, height = 1, linewidth = 0.5, colour = \"white\") +\n  scale_fill_viridis_d(name = \"Antall ganger\\nenheten er valgt\", \n                       option = \"C\", direction = -1, na.translate = F) +\n  coord_fixed()\n \n#ggsave(\"img/antallGangerValgt.png\")\n\n\n\n\n\n\nFigure 6.8: Figure showing the selected population units, and how many of the in total five nestes samples they were included in. Legend title translates to “Number of times selected”.",
    "crumbs": [
      "Spatial sampling",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Simulation</span>"
    ]
  },
  {
    "objectID": "030_simulation.html#sec-what",
    "href": "030_simulation.html#sec-what",
    "title": "\n6  Simulation\n",
    "section": "\n6.8 Whats the pi?",
    "text": "6.8 Whats the pi?\nThe inclusion probabilities are accumulated for each iteration of sampling. the initial pps sampling usually has the lowest probabilities, esp. if the population &gt;&gt; sample. We can access the accumulated probabilities in the output\n\nh_samples$mysample_100$accumulated_pi[1:10]\n\n\nh_samples$mysample_20$accumulated_pi[1:10]",
    "crumbs": [
      "Spatial sampling",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Simulation</span>"
    ]
  },
  {
    "objectID": "030_simulation.html#create-function",
    "href": "030_simulation.html#create-function",
    "title": "\n6  Simulation\n",
    "section": "\n6.9 Create function",
    "text": "6.9 Create function\nBased on the above, and with the help of ChatGPT [version 5), I created a contained function nested_balanced_samples that we can source in subsequent chapters. You can fund in under R/ in the github repo.\n\nsource(here::here(\"R/nested_balanced_samples.R\"))",
    "crumbs": [
      "Spatial sampling",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Simulation</span>"
    ]
  },
  {
    "objectID": "031_wetland_sample.html",
    "href": "031_wetland_sample.html",
    "title": "\n7  Wetland samples\n",
    "section": "",
    "text": "7.1 Get and prepare sampling frame\nFirst we need to fetch the sampling frame from the database.\ncon &lt;- DBI::dbConnect(drv = RPostgres::Postgres(), host = \"t2lippgsql03\", dbname = \"ano_moduler\")\n\npop &lt;- dplyr::tbl(con, dbplyr::in_schema(\"sampling_frames\", \"samplingframe_vaatmark_2025\")) |&gt;\n  dplyr::collect()\n\n# helper to get a small subset of the data for testing\nsmall &lt;- function(data = pop) data |&gt;  slice_sample(n = 10000)\nThis dataset (sampling frame) is 1350918 long. Table 7.1 show just the first ten rows. We have two spreading variables: an easting ost, a northing nord. We also have sampling unit ID’s in ssbid, and two auxiliary variable for balancing: mean_slope and abi. The stratification variable we need to create based on seksjon_kode and sone_kode. We also need to create an area variable based on the sum of r_hav and r_ferskvann.\nTable 7.1\n\npop |&gt;\n  slice_head(n = 10) |&gt;\n  DT::datatable()\npop &lt;- pop |&gt;\n  rename(Northing = nord,\n         Easting = ost) |&gt;\n  mutate(area = 100 - r_hav - r_ferskvann,\n         stratum = case_when(\n           seksjon_kode %in% c(\"6SE-3\", \"6SE-4\", \"6SE-5\") & sone_kode %in% c(\"6SO-1\", \"6SO-2\") ~  1,\n           seksjon_kode %in% c(\"6SE-2\") & sone_kode %in% c(\"6SO-1\", \"6SO-2\") ~  2,\n           seksjon_kode %in% c(\"6SE-1\") & sone_kode %in% c(\"6SO-1\", \"6SO-2\") ~  3,\n           seksjon_kode %in% c(\"6SE-1\") & sone_kode %in% c(\"6SO-3\", \"6SO-4\", \"6SO-5\") ~  4,\n           seksjon_kode %in% c(\"6SE-2\") & sone_kode %in% c(\"6SO-3\") ~  5,\n           seksjon_kode %in% c(\"6SE-2\") & sone_kode %in% c(\"6SO-4\", \"6SO-5\") ~  6,\n           seksjon_kode %in% c(\"6SE-3\", \"6SE-4\", \"6SE-5\") & sone_kode %in% c(\"6SO-3\") ~  7,\n           seksjon_kode %in% c(\"6SE-3\", \"6SE-4\") & sone_kode %in% c(\"6SO-4\", \"6SO-5\") ~  8,\n           seksjon_kode %in% c(\"6SE-5\") & sone_kode %in% c(\"6SO-4\", \"6SO-5\") ~  9,\n           .default = 99\n           ))\nThere is one or a few cases of negative areas. I will just set those tpo be zeros.\n# min(pop$area) #negative\n# max(pop$area) #OK\npop &lt;- pop |&gt;\n  mutate(area = case_when(\n    area &lt; 0 ~ 0,\n    .default = area\n  ))\nThere is a problem with NAs in the bioclimatic zones and regions:\nTable 7.2: Number of population units (SSB500 squares) in each strata.\n\npop |&gt;\n  group_by(stratum) |&gt;\n  summarise(n = n())\n\n# A tibble: 10 × 2\n   stratum      n\n     &lt;dbl&gt;  &lt;int&gt;\n 1       1 137782\n 2       2 124391\n 3       3  43099\n 4       4  45527\n 5       5 108498\n 6       6 130314\n 7       7 136524\n 8       8 504568\n 9       9 110879\n10      99   9336\nWe have 9336 population units that we don’t know the strata for. Let’s see if there is a pattern to where these units are found. The first check below tells us that stratum 99 is a consequence of NA’s on both the sone and section.\npop |&gt;\n  filter(stratum == 99) |&gt;\n  tidyr::unite(combo, c(seksjon_kode, sone_kode)) |&gt;\n  distinct(combo)\n\n# A tibble: 1 × 1\n  combo\n  &lt;chr&gt;\n1 NA_NA\nThen Figure 7.1 tells us that the population units where we lack data on bioclimatic regions are mainly along the coast (a lot of ocean), but also close to freshwater. This is a problem, as it will lead to undersampling of areas close to water. However, we cannot solve this now, and rumors have it that the bioclimatic zone and regions dataset will get updated very soon, so then we can also update (rerun) the sampling pipeline.\nThe same problem seems to also be for abi, that we have false zero values along the coast or along water. We know this from visual checks, but also from Figure 7.1 where we see that abi is always zero, at the same time that r_bebygd_samf is not.\npop |&gt;\n  filter(stratum == 99) |&gt;\n  pivot_longer(cols = c(r_hav, r_ferskvann, abi, mean_slope, r_bebygd_samf, r_sno_isbre)) |&gt;\n  ggplot()+\n  geom_dotplot(aes(value))+\n  facet_wrap(.~name)+\n  theme_bw()\n\n\n\n\n\n\nFigure 7.1: Distrbutions for selected variables for whick we lack data on stratum (bioclimatic region).",
    "crumbs": [
      "Spatial sampling",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Wetland samples</span>"
    ]
  },
  {
    "objectID": "031_wetland_sample.html#sample",
    "href": "031_wetland_sample.html#sample",
    "title": "\n7  Wetland samples\n",
    "section": "\n7.2 Sample",
    "text": "7.2 Sample\nFirst we subset the columns, and run a test on 10 000 rows.\n\npop &lt;- pop |&gt;\n  select(ssbid, Easting, Northing, area, abi, mean_slope, stratum) |&gt;\n    filter(stratum != 99)\n\nwet_small &lt;- small()\n\n\ntestRun &lt;- spaceR::nested_balanced(\n  samplingFrame = wet_small,\n  n_seq = seq(50, 20, -10),\n  id_col = \"ssbid\",\n  stratum_col = \"stratum\",\n  easting_col = \"Easting\",\n  northing_col = \"Northing\",\n  area_col = \"area\",\n  xbal_formula = ~ abi + mean_slope -1,\n  exclude_offset = 1e+09,\n  return_dataframe = TRUE,\n  out_name = \"testSample\"\n)\n\nSampling with n =  50\n\n\nSampling with n =  40\n\n\nSampling with n =  30\n\n\nSampling with n =  20\n\n\nThe test went fine, with no errors.\nThe returned object testRun which is a list of lists.\n\nnames(testRun)\n\n[1] \"testSample_50\" \"testSample_40\" \"testSample_30\" \"testSample_20\"\n\n\nThen we try the same on the entire sampling population\n\ntictoc::tic()\nwetlands &lt;- spaceR::nested_balanced(\n  samplingFrame = pop,\n  n_seq = seq(5000, 100, -10),\n  id_col = \"ssbid\",\n  stratum_col = \"stratum\",\n  easting_col = \"Easting\",\n  northing_col = \"Northing\",\n  area_col = \"area\",\n  xbal_formula = ~ abi + mean_slope -1,\n  exclude_offset = 1e+09,\n  return_dataframe = FALSE,\n  out_name = \"wetlands1\",\n)\ntictoc::toc()\n# 1012.165 sec elapsed\n#saveRDS(wetlands, \"data/wetlands.rds\")\n\nNow I will combine all these lists of lists into something more manageble, like a dataframe.\n\nsample_names &lt;- names(wetlands)\nsample_sizes &lt;- gsub(\"wetlands1_\", \"\", sample_names) |&gt; as.numeric()\nall_ids &lt;- wetlands$wetlands1_5000$ID\n\n# Initialize empty matrix with NA\nprob_matrix &lt;- matrix(NA_real_, nrow = length(all_ids), ncol = length(sample_names),\n                      dimnames = list(all_ids, sample_sizes))\n\n# Fill in probabilities where IDs occur\nfor (i in seq_along(wetlands)) {\n  w &lt;- wetlands[[i]]\n  id_match &lt;- match(w$ID, all_ids)\n  prob_matrix[id_match, i] &lt;- w$prob\n}\n\n# create column with IDs\nprob_matrix &lt;-  prob_matrix |&gt;\n  as.data.frame() |&gt;\n  rownames_to_column(\"ssbid\") |&gt;\n  as_tibble()\n\n# append aux data\nprob_matrix &lt;- pop |&gt;\n  right_join(prob_matrix, by = join_by(ssbid))\n\nhead(prob_matrix)\n\nIn the code below I check that the proportion stay stable after the initial pps, and the do. Compared to the full population, we have reduced the proportion of sampling units that have less then 100 or 50% or area by approx. 9 and 71%, respectively.\n\nprob_matrix |&gt;\n  select(ssbid, area, \"5000\", \"3000\", \"100\") |&gt;\n  pivot_longer(cols = c(\"5000\", \"3000\", \"100\"),\n               values_drop_na = TRUE) |&gt;\n  group_by(name) |&gt;\n  summarise(prop_below_100 = mean(area &lt; 100, na.rm=T),\n            prop_below_50 = mean(area &lt; 50, na.rm=T)) |&gt;\n  rbind(\n    pop |&gt;\n  summarise(prop_below_100 = mean(area &lt; 100, na.rm=T),\n            prop_below_50 = mean(area &lt; 50, na.rm=T)) |&gt;\n    add_column(\"name\" = \"Population\", .before = 1)\n  ) |&gt;\n  kable()",
    "crumbs": [
      "Spatial sampling",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Wetland samples</span>"
    ]
  },
  {
    "objectID": "031_wetland_sample.html#evaluate-balance",
    "href": "031_wetland_sample.html#evaluate-balance",
    "title": "\n7  Wetland samples\n",
    "section": "\n7.3 Evaluate balance",
    "text": "7.3 Evaluate balance\nHere we should ideally evaluate the balance of the sample against a simple random sample, for example using BalancedSampling::sb(), but we don’t have time to to this now unfortunately.",
    "crumbs": [
      "Spatial sampling",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Wetland samples</span>"
    ]
  },
  {
    "objectID": "031_wetland_sample.html#write-to-db",
    "href": "031_wetland_sample.html#write-to-db",
    "title": "\n7  Wetland samples\n",
    "section": "\n7.4 Write to db",
    "text": "7.4 Write to db\n\n7.4.1 Setup sample schema\nThis part sets up the schema, and is only done ones.\n\nschema_ur &lt;- \"CREATE SCHEMA samples\"\ndbSendQuery(con, schema_ur) \n\nWrite queries to grant read only access to all.\n\npriv &lt;- \"ALTER DEFAULT PRIVILEGES IN SCHEMA samples GRANT SELECT ON TABLES TO ag_pgsql_ano_moduler_ro\"\npriv2 &lt;- \"ALTER DEFAULT PRIVILEGES IN SCHEMA samples GRANT SELECT ON TABLES TO ag_pgsql_ano_moduler_rw\"\npriv3 &lt;- \"ALTER DEFAULT PRIVILEGES IN SCHEMA samples GRANT SELECT ON TABLES TO ag_pgsql_ano_moduler_admin\"\npriv4 &lt;- \"GRANT USAGE ON SCHEMA samples  TO ag_pgsql_ano_moduler_admin\"\npriv5 &lt;- \"GRANT USAGE ON SCHEMA samples  TO ag_pgsql_ano_moduler_rw\"\npriv6 &lt;- \"GRANT USAGE ON SCHEMA samples  TO ag_pgsql_ano_moduler_ro\"\n\ndbSendStatement(con, priv)\ndbSendStatement(con, priv2)\ndbSendStatement(con, priv3)\ndbSendStatement(con, priv4)\ndbSendStatement(con, priv5)\ndbSendStatement(con, priv6)\n\n\n7.4.2 Prepare and export\n\nmyIDs &lt;- uuid::UUIDgenerate(n = nrow(prob_matrix))\nwetlands_db &lt;- prob_matrix |&gt;\n  mutate(wetlands_sample_unit_id = myIDs,\n         .after = 1) |&gt;\n  select(-Easting, -Northing, -area, -abi, -mean_slope, -stratum)\n\nwrite_csv(wetlands_db, \"data/wetland_samples.csv\")\n\n\nq1 &lt;- 'create table samples.vaatmark_2025 (\nwetlands_sample_unit_ID varchar(50) PRIMARY KEY,\nssbid varchar(50),\n  \"5000\" numeric(24,23),\n  \"4990\" numeric(24,23),\n  \"4980\" numeric(24,23),\n  \"4970\" numeric(24,23),\n  \"4960\" numeric(24,23),\n  \"4950\" numeric(24,23),\n  \"4940\" numeric(24,23),\n  \"4930\" numeric(24,23),\n  \"4920\" numeric(24,23),\n  \"4910\" numeric(24,23),\n  \"4900\" numeric(24,23),\n  \"4890\" numeric(24,23),\n  \"4880\" numeric(24,23),\n  \"4870\" numeric(24,23),\n  \"4860\" numeric(24,23),\n  \"4850\" numeric(24,23),\n  \"4840\" numeric(24,23),\n  \"4830\" numeric(24,23),\n  \"4820\" numeric(24,23),\n  \"4810\" numeric(24,23),\n  \"4800\" numeric(24,23),\n  \"4790\" numeric(24,23),\n  \"4780\" numeric(24,23),\n  \"4770\" numeric(24,23),\n  \"4760\" numeric(24,23),\n  \"4750\" numeric(24,23),\n  \"4740\" numeric(24,23),\n  \"4730\" numeric(24,23),\n  \"4720\" numeric(24,23),\n  \"4710\" numeric(24,23),\n  \"4700\" numeric(24,23),\n  \"4690\" numeric(24,23),\n  \"4680\" numeric(24,23),\n  \"4670\" numeric(24,23),\n  \"4660\" numeric(24,23),\n  \"4650\" numeric(24,23),\n  \"4640\" numeric(24,23),\n  \"4630\" numeric(24,23),\n  \"4620\" numeric(24,23),\n  \"4610\" numeric(24,23),\n  \"4600\" numeric(24,23),\n  \"4590\" numeric(24,23),\n  \"4580\" numeric(24,23),\n  \"4570\" numeric(24,23),\n  \"4560\" numeric(24,23),\n  \"4550\" numeric(24,23),\n  \"4540\" numeric(24,23),\n  \"4530\" numeric(24,23),\n  \"4520\" numeric(24,23),\n  \"4510\" numeric(24,23),\n  \"4500\" numeric(24,23),\n  \"4490\" numeric(24,23),\n  \"4480\" numeric(24,23),\n  \"4470\" numeric(24,23),\n  \"4460\" numeric(24,23),\n  \"4450\" numeric(24,23),\n  \"4440\" numeric(24,23),\n  \"4430\" numeric(24,23),\n  \"4420\" numeric(24,23),\n  \"4410\" numeric(24,23),\n  \"4400\" numeric(24,23),\n  \"4390\" numeric(24,23),\n  \"4380\" numeric(24,23),\n  \"4370\" numeric(24,23),\n  \"4360\" numeric(24,23),\n  \"4350\" numeric(24,23),\n  \"4340\" numeric(24,23),\n  \"4330\" numeric(24,23),\n  \"4320\" numeric(24,23),\n  \"4310\" numeric(24,23),\n  \"4300\" numeric(24,23),\n  \"4290\" numeric(24,23),\n  \"4280\" numeric(24,23),\n  \"4270\" numeric(24,23),\n  \"4260\" numeric(24,23),\n  \"4250\" numeric(24,23),\n  \"4240\" numeric(24,23),\n  \"4230\" numeric(24,23),\n  \"4220\" numeric(24,23),\n  \"4210\" numeric(24,23),\n  \"4200\" numeric(24,23),\n  \"4190\" numeric(24,23),\n  \"4180\" numeric(24,23),\n  \"4170\" numeric(24,23),\n  \"4160\" numeric(24,23),\n  \"4150\" numeric(24,23),\n  \"4140\" numeric(24,23),\n  \"4130\" numeric(24,23),\n  \"4120\" numeric(24,23),\n  \"4110\" numeric(24,23),\n  \"4100\" numeric(24,23),\n  \"4090\" numeric(24,23),\n  \"4080\" numeric(24,23),\n  \"4070\" numeric(24,23),\n  \"4060\" numeric(24,23),\n  \"4050\" numeric(24,23),\n  \"4040\" numeric(24,23),\n  \"4030\" numeric(24,23),\n  \"4020\" numeric(24,23),\n  \"4010\" numeric(24,23),\n  \"4000\" numeric(24,23),\n  \"3990\" numeric(24,23),\n  \"3980\" numeric(24,23),\n  \"3970\" numeric(24,23),\n  \"3960\" numeric(24,23),\n  \"3950\" numeric(24,23),\n  \"3940\" numeric(24,23),\n  \"3930\" numeric(24,23),\n  \"3920\" numeric(24,23),\n  \"3910\" numeric(24,23),\n  \"3900\" numeric(24,23),\n  \"3890\" numeric(24,23),\n  \"3880\" numeric(24,23),\n  \"3870\" numeric(24,23),\n  \"3860\" numeric(24,23),\n  \"3850\" numeric(24,23),\n  \"3840\" numeric(24,23),\n  \"3830\" numeric(24,23),\n  \"3820\" numeric(24,23),\n  \"3810\" numeric(24,23),\n  \"3800\" numeric(24,23),\n  \"3790\" numeric(24,23),\n  \"3780\" numeric(24,23),\n  \"3770\" numeric(24,23),\n  \"3760\" numeric(24,23),\n  \"3750\" numeric(24,23),\n  \"3740\" numeric(24,23),\n  \"3730\" numeric(24,23),\n  \"3720\" numeric(24,23),\n  \"3710\" numeric(24,23),\n  \"3700\" numeric(24,23),\n  \"3690\" numeric(24,23),\n  \"3680\" numeric(24,23),\n  \"3670\" numeric(24,23),\n  \"3660\" numeric(24,23),\n  \"3650\" numeric(24,23),\n  \"3640\" numeric(24,23),\n  \"3630\" numeric(24,23),\n  \"3620\" numeric(24,23),\n  \"3610\" numeric(24,23),\n  \"3600\" numeric(24,23),\n  \"3590\" numeric(24,23),\n  \"3580\" numeric(24,23),\n  \"3570\" numeric(24,23),\n  \"3560\" numeric(24,23),\n  \"3550\" numeric(24,23),\n  \"3540\" numeric(24,23),\n  \"3530\" numeric(24,23),\n  \"3520\" numeric(24,23),\n  \"3510\" numeric(24,23),\n  \"3500\" numeric(24,23),\n  \"3490\" numeric(24,23),\n  \"3480\" numeric(24,23),\n  \"3470\" numeric(24,23),\n  \"3460\" numeric(24,23),\n  \"3450\" numeric(24,23),\n  \"3440\" numeric(24,23),\n  \"3430\" numeric(24,23),\n  \"3420\" numeric(24,23),\n  \"3410\" numeric(24,23),\n  \"3400\" numeric(24,23),\n  \"3390\" numeric(24,23),\n  \"3380\" numeric(24,23),\n  \"3370\" numeric(24,23),\n  \"3360\" numeric(24,23),\n  \"3350\" numeric(24,23),\n  \"3340\" numeric(24,23),\n  \"3330\" numeric(24,23),\n  \"3320\" numeric(24,23),\n  \"3310\" numeric(24,23),\n  \"3300\" numeric(24,23),\n  \"3290\" numeric(24,23),\n  \"3280\" numeric(24,23),\n  \"3270\" numeric(24,23),\n  \"3260\" numeric(24,23),\n  \"3250\" numeric(24,23),\n  \"3240\" numeric(24,23),\n  \"3230\" numeric(24,23),\n  \"3220\" numeric(24,23),\n  \"3210\" numeric(24,23),\n  \"3200\" numeric(24,23),\n  \"3190\" numeric(24,23),\n  \"3180\" numeric(24,23),\n  \"3170\" numeric(24,23),\n  \"3160\" numeric(24,23),\n  \"3150\" numeric(24,23),\n  \"3140\" numeric(24,23),\n  \"3130\" numeric(24,23),\n  \"3120\" numeric(24,23),\n  \"3110\" numeric(24,23),\n  \"3100\" numeric(24,23),\n  \"3090\" numeric(24,23),\n  \"3080\" numeric(24,23),\n  \"3070\" numeric(24,23),\n  \"3060\" numeric(24,23),\n  \"3050\" numeric(24,23),\n  \"3040\" numeric(24,23),\n  \"3030\" numeric(24,23),\n  \"3020\" numeric(24,23),\n  \"3010\" numeric(24,23),\n  \"3000\" numeric(24,23),\n  \"2990\" numeric(24,23),\n  \"2980\" numeric(24,23),\n  \"2970\" numeric(24,23),\n  \"2960\" numeric(24,23),\n  \"2950\" numeric(24,23),\n  \"2940\" numeric(24,23),\n  \"2930\" numeric(24,23),\n  \"2920\" numeric(24,23),\n  \"2910\" numeric(24,23),\n  \"2900\" numeric(24,23),\n  \"2890\" numeric(24,23),\n  \"2880\" numeric(24,23),\n  \"2870\" numeric(24,23),\n  \"2860\" numeric(24,23),\n  \"2850\" numeric(24,23),\n  \"2840\" numeric(24,23),\n  \"2830\" numeric(24,23),\n  \"2820\" numeric(24,23),\n  \"2810\" numeric(24,23),\n  \"2800\" numeric(24,23),\n  \"2790\" numeric(24,23),\n  \"2780\" numeric(24,23),\n  \"2770\" numeric(24,23),\n  \"2760\" numeric(24,23),\n  \"2750\" numeric(24,23),\n  \"2740\" numeric(24,23),\n  \"2730\" numeric(24,23),\n  \"2720\" numeric(24,23),\n  \"2710\" numeric(24,23),\n  \"2700\" numeric(24,23),\n  \"2690\" numeric(24,23),\n  \"2680\" numeric(24,23),\n  \"2670\" numeric(24,23),\n  \"2660\" numeric(24,23),\n  \"2650\" numeric(24,23),\n  \"2640\" numeric(24,23),\n  \"2630\" numeric(24,23),\n  \"2620\" numeric(24,23),\n  \"2610\" numeric(24,23),\n  \"2600\" numeric(24,23),\n  \"2590\" numeric(24,23),\n  \"2580\" numeric(24,23),\n  \"2570\" numeric(24,23),\n  \"2560\" numeric(24,23),\n  \"2550\" numeric(24,23),\n  \"2540\" numeric(24,23),\n  \"2530\" numeric(24,23),\n  \"2520\" numeric(24,23),\n  \"2510\" numeric(24,23),\n  \"2500\" numeric(24,23),\n  \"2490\" numeric(24,23),\n  \"2480\" numeric(24,23),\n  \"2470\" numeric(24,23),\n  \"2460\" numeric(24,23),\n  \"2450\" numeric(24,23),\n  \"2440\" numeric(24,23),\n  \"2430\" numeric(24,23),\n  \"2420\" numeric(24,23),\n  \"2410\" numeric(24,23),\n  \"2400\" numeric(24,23),\n  \"2390\" numeric(24,23),\n  \"2380\" numeric(24,23),\n  \"2370\" numeric(24,23),\n  \"2360\" numeric(24,23),\n  \"2350\" numeric(24,23),\n  \"2340\" numeric(24,23),\n  \"2330\" numeric(24,23),\n  \"2320\" numeric(24,23),\n  \"2310\" numeric(24,23),\n  \"2300\" numeric(24,23),\n  \"2290\" numeric(24,23),\n  \"2280\" numeric(24,23),\n  \"2270\" numeric(24,23),\n  \"2260\" numeric(24,23),\n  \"2250\" numeric(24,23),\n  \"2240\" numeric(24,23),\n  \"2230\" numeric(24,23),\n  \"2220\" numeric(24,23),\n  \"2210\" numeric(24,23),\n  \"2200\" numeric(24,23),\n  \"2190\" numeric(24,23),\n  \"2180\" numeric(24,23),\n  \"2170\" numeric(24,23),\n  \"2160\" numeric(24,23),\n  \"2150\" numeric(24,23),\n  \"2140\" numeric(24,23),\n  \"2130\" numeric(24,23),\n  \"2120\" numeric(24,23),\n  \"2110\" numeric(24,23),\n  \"2100\" numeric(24,23),\n  \"2090\" numeric(24,23),\n  \"2080\" numeric(24,23),\n  \"2070\" numeric(24,23),\n  \"2060\" numeric(24,23),\n  \"2050\" numeric(24,23),\n  \"2040\" numeric(24,23),\n  \"2030\" numeric(24,23),\n  \"2020\" numeric(24,23),\n  \"2010\" numeric(24,23),\n  \"2000\" numeric(24,23),\n  \"1990\" numeric(24,23),\n  \"1980\" numeric(24,23),\n  \"1970\" numeric(24,23),\n  \"1960\" numeric(24,23),\n  \"1950\" numeric(24,23),\n  \"1940\" numeric(24,23),\n  \"1930\" numeric(24,23),\n  \"1920\" numeric(24,23),\n  \"1910\" numeric(24,23),\n  \"1900\" numeric(24,23),\n  \"1890\" numeric(24,23),\n  \"1880\" numeric(24,23),\n  \"1870\" numeric(24,23),\n  \"1860\" numeric(24,23),\n  \"1850\" numeric(24,23),\n  \"1840\" numeric(24,23),\n  \"1830\" numeric(24,23),\n  \"1820\" numeric(24,23),\n  \"1810\" numeric(24,23),\n  \"1800\" numeric(24,23),\n  \"1790\" numeric(24,23),\n  \"1780\" numeric(24,23),\n  \"1770\" numeric(24,23),\n  \"1760\" numeric(24,23),\n  \"1750\" numeric(24,23),\n  \"1740\" numeric(24,23),\n  \"1730\" numeric(24,23),\n  \"1720\" numeric(24,23),\n  \"1710\" numeric(24,23),\n  \"1700\" numeric(24,23),\n  \"1690\" numeric(24,23),\n  \"1680\" numeric(24,23),\n  \"1670\" numeric(24,23),\n  \"1660\" numeric(24,23),\n  \"1650\" numeric(24,23),\n  \"1640\" numeric(24,23),\n  \"1630\" numeric(24,23),\n  \"1620\" numeric(24,23),\n  \"1610\" numeric(24,23),\n  \"1600\" numeric(24,23),\n  \"1590\" numeric(24,23),\n  \"1580\" numeric(24,23),\n  \"1570\" numeric(24,23),\n  \"1560\" numeric(24,23),\n  \"1550\" numeric(24,23),\n  \"1540\" numeric(24,23),\n  \"1530\" numeric(24,23),\n  \"1520\" numeric(24,23),\n  \"1510\" numeric(24,23),\n  \"1500\" numeric(24,23),\n  \"1490\" numeric(24,23),\n  \"1480\" numeric(24,23),\n  \"1470\" numeric(24,23),\n  \"1460\" numeric(24,23),\n  \"1450\" numeric(24,23),\n  \"1440\" numeric(24,23),\n  \"1430\" numeric(24,23),\n  \"1420\" numeric(24,23),\n  \"1410\" numeric(24,23),\n  \"1400\" numeric(24,23),\n  \"1390\" numeric(24,23),\n  \"1380\" numeric(24,23),\n  \"1370\" numeric(24,23),\n  \"1360\" numeric(24,23),\n  \"1350\" numeric(24,23),\n  \"1340\" numeric(24,23),\n  \"1330\" numeric(24,23),\n  \"1320\" numeric(24,23),\n  \"1310\" numeric(24,23),\n  \"1300\" numeric(24,23),\n  \"1290\" numeric(24,23),\n  \"1280\" numeric(24,23),\n  \"1270\" numeric(24,23),\n  \"1260\" numeric(24,23),\n  \"1250\" numeric(24,23),\n  \"1240\" numeric(24,23),\n  \"1230\" numeric(24,23),\n  \"1220\" numeric(24,23),\n  \"1210\" numeric(24,23),\n  \"1200\" numeric(24,23),\n  \"1190\" numeric(24,23),\n  \"1180\" numeric(24,23),\n  \"1170\" numeric(24,23),\n  \"1160\" numeric(24,23),\n  \"1150\" numeric(24,23),\n  \"1140\" numeric(24,23),\n  \"1130\" numeric(24,23),\n  \"1120\" numeric(24,23),\n  \"1110\" numeric(24,23),\n  \"1100\" numeric(24,23),\n  \"1090\" numeric(24,23),\n  \"1080\" numeric(24,23),\n  \"1070\" numeric(24,23),\n  \"1060\" numeric(24,23),\n  \"1050\" numeric(24,23),\n  \"1040\" numeric(24,23),\n  \"1030\" numeric(24,23),\n  \"1020\" numeric(24,23),\n  \"1010\" numeric(24,23),\n  \"1000\" numeric(24,23),\n   \"990\" numeric(24,23),\n   \"980\" numeric(24,23),\n   \"970\" numeric(24,23),\n   \"960\" numeric(24,23),\n   \"950\" numeric(24,23),\n   \"940\" numeric(24,23),\n   \"930\" numeric(24,23),\n   \"920\" numeric(24,23),\n   \"910\" numeric(24,23),\n   \"900\" numeric(24,23),\n   \"890\" numeric(24,23),\n   \"880\" numeric(24,23),\n   \"870\" numeric(24,23),\n   \"860\" numeric(24,23),\n   \"850\" numeric(24,23),\n   \"840\" numeric(24,23),\n   \"830\" numeric(24,23),\n   \"820\" numeric(24,23),\n   \"810\" numeric(24,23),\n   \"800\" numeric(24,23),\n   \"790\" numeric(24,23),\n   \"780\" numeric(24,23),\n   \"770\" numeric(24,23),\n   \"760\" numeric(24,23),\n   \"750\" numeric(24,23),\n   \"740\" numeric(24,23),\n   \"730\" numeric(24,23),\n   \"720\" numeric(24,23),\n   \"710\" numeric(24,23),\n   \"700\" numeric(24,23),\n   \"690\" numeric(24,23),\n   \"680\" numeric(24,23),\n   \"670\" numeric(24,23),\n   \"660\" numeric(24,23),\n   \"650\" numeric(24,23),\n   \"640\" numeric(24,23),\n   \"630\" numeric(24,23),\n   \"620\" numeric(24,23),\n   \"610\" numeric(24,23),\n   \"600\" numeric(24,23),\n   \"590\" numeric(24,23),\n   \"580\" numeric(24,23),\n   \"570\" numeric(24,23),\n   \"560\" numeric(24,23),\n   \"550\" numeric(24,23),\n   \"540\" numeric(24,23),\n   \"530\" numeric(24,23),\n   \"520\" numeric(24,23),\n   \"510\" numeric(24,23),\n   \"500\" numeric(24,23),\n   \"490\" numeric(24,23),\n   \"480\" numeric(24,23),\n   \"470\" numeric(24,23),\n   \"460\" numeric(24,23),\n   \"450\" numeric(24,23),\n   \"440\" numeric(24,23),\n   \"430\" numeric(24,23),\n   \"420\" numeric(24,23),\n   \"410\" numeric(24,23),\n   \"400\" numeric(24,23),\n   \"390\" numeric(24,23),\n   \"380\" numeric(24,23),\n   \"370\" numeric(24,23),\n   \"360\" numeric(24,23),\n   \"350\" numeric(24,23),\n   \"340\" numeric(24,23),\n   \"330\" numeric(24,23),\n   \"320\" numeric(24,23),\n   \"310\" numeric(24,23),\n   \"300\" numeric(24,23),\n   \"290\" numeric(24,23),\n   \"280\" numeric(24,23),\n   \"270\" numeric(24,23),\n   \"260\" numeric(24,23),\n   \"250\" numeric(24,23),\n   \"240\" numeric(24,23),\n   \"230\" numeric(24,23),\n   \"220\" numeric(24,23),\n   \"210\" numeric(24,23),\n   \"200\" numeric(24,23),\n   \"190\" numeric(24,23),\n   \"180\" numeric(24,23),\n   \"170\" numeric(24,23),\n   \"160\" numeric(24,23),\n   \"150\" numeric(24,23),\n   \"140\" numeric(24,23),\n   \"130\" numeric(24,23),\n   \"120\" numeric(24,23),\n   \"110\" numeric(24,23),\n   \"100\" numeric(24,23),\n  CONSTRAINT fk_vaatmark_ssbid\n    FOREIGN KEY (ssbid)\n      REFERENCES ssb_grids.ssb_500 (ssbid)\n    DEFERRABLE INITIALLY DEFERRED\n);'\n\n\n# indices makes the database work faster. It should be added to all tables that are looked up frequently\nq2 &lt;- \"create index on samples.vaatmark_2025 using btree(ssbid);\"\nq3 &lt;- \"create index on samples.vaatmark_2025 using btree(wetlands_sample_unit_ID);\"\n\n\n# sending the queries:\ndbSendStatement(con, q1)\ndbSendStatement(con, q2)\ndbSendStatement(con, q3)\n\n\nwrite_sf(wetlands_db, dsn = con,\n         layer = Id(schema = \"samples\", table = \"vaatmark_2025\"), \n         append = T)\n\nTesting that the data is there:\n\ndplyr::tbl(con, dbplyr::in_schema(\"samples\", \"vaatmark_2025\")) |&gt;\n  #dplyr::slice(n = 8) |&gt;\n  select(ssbid, \"5000\", \"3000\", \"100\")\n\n# Source:   SQL [?? x 4]\n# Database: postgres  [anders.kolstad@t2lippgsql03:5432/ano_moduler]\n   ssbid          `5000`   `3000` `100`\n   &lt;chr&gt;           &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;\n 1 22495006596000 0.0155  0.00932    NA\n 2 23775006793000 0.0390 NA          NA\n 3 22970006534000 0.0382  0.0229     NA\n 4 22995006534000 0.0366 NA          NA\n 5 22960006536000 0.0410 NA          NA\n 6 22995006537000 0.0410  0.0246     NA\n 7 22980006541500 0.0410 NA          NA\n 8 22990006544000 0.0409  0.0246     NA\n 9 22980006545000 0.0410  0.0246     NA\n10 22730006545500 0.0398  0.0239     NA\n# ℹ more rows",
    "crumbs": [
      "Spatial sampling",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Wetland samples</span>"
    ]
  },
  {
    "objectID": "021_readFromDb.html#collect-and-join-data",
    "href": "021_readFromDb.html#collect-and-join-data",
    "title": "\n5  Read data from the database\n",
    "section": "\n5.2 Collect and join data",
    "text": "5.2 Collect and join data\nThis data only exists remotely still. We need to use collect to bring it down to our local machine. At the same time we can use dplyr pipelines to filter the data.\n\ncoast_sample |&gt;\n  dplyr::mutate(id = row_number()) |&gt;\n  dplyr::slice_min(n = 8, order_by = id) |&gt;\n  dplyr::collect()\n\n# A tibble: 8 × 3\n  havstrand_id                         ssbid               id\n  &lt;chr&gt;                                &lt;chr&gt;          &lt;int64&gt;\n1 4de8478a-d2c8-4730-83dd-12ac25d4fa07 22495006594500       1\n2 bc9a80d7-c434-47e9-a89a-7f7a3f59f625 22495006595000       2\n3 ce7166b6-cb49-499a-a99f-9da0e274abd8 22495006595500       3\n4 e56036c5-7fe4-461a-a022-44367119acac 22495006596000       4\n5 1f592dac-2fb7-49e5-9aa9-fc0cf9e9184f 22490006596500       5\n6 20394fe8-305c-4c39-add9-f013295ca932 22495006596500       6\n7 3d3ffe94-0400-4c1c-9a0f-62f4d6c29c4d 22490006597000       7\n8 4d199401-dd20-48ef-b6c6-8de00b9eb9a4 22495006597000       8\n\n\nWe can also get the geometries, which are stored in the foreign table. To do this we can use the dm package.\nWe first create a dm object from the connection object. Then we can view data easily.\n\ndm &lt;- dm::dm_from_con(con,\n  table_names = c(\n    \"ssb_500\",\n    \"vaatmark_2025\"\n  ),\n  learn_keys = T\n)\ndm\n\n── Table source ────────────────────────────────────────────────────────────────\nsrc:  postgres  [anders.kolstad@t2lippgsql03:5432/ano_moduler]\n── Metadata ────────────────────────────────────────────────────────────────────\nTables: `ssb_500`, `vaatmark_2025`\nColumns: 495\nPrimary keys: 2\nForeign keys: 1\n\n\nDM learns the relationships between tables by reading the constraints. First I can check that the amount of constraints is good.\n\ndm::dm_get_all_fks(dm)\n\n# A tibble: 1 × 5\n  child_table   child_fk_cols parent_table parent_key_cols on_delete\n  &lt;chr&gt;         &lt;keys&gt;        &lt;chr&gt;        &lt;keys&gt;          &lt;chr&gt;    \n1 vaatmark_2025 ssbid         ssb_500      ssbid           no_action\n\n\n\ndm |&gt;\n  dm::dm_set_colors(\n    darkgreen = vaatmark_2025, \n    darkblue = ssb_500) |&gt;\n  dm::dm_draw() \n\n\n\n\n\nThen we can read the data back, including the geometries from the parent table. The geometries are stores (or at least returned) as hex-encoded WKB/EWKB, and we need to convert then to sfc first, before we can turn them into sf.\n\ndm2 &lt;- dm |&gt;\n  dm::dm_flatten_to_tbl(vaatmark_2025,\n                    .recursive = TRUE) |&gt;\n  mutate(geom_wkb = dbplyr::sql(\"ST_AsBinary(geom)\")) |&gt;\n  select(-geom) |&gt;\n  collect() |&gt;\n  mutate(geom = sf::st_as_sfc(geom_wkb, crs = 25833)) |&gt;\n  sf::st_as_sf(sf_column_name = \"geom\")\n\n\npath &lt;- \"/data/Egenutvikling/41001581_egenutvikling_anders_kolstad/ANO\"\nst_write(dm2, paste0(path, \"/vaatmark_2025.gpkg\"), append = FALSE)\n\n\nlibrary(tmap)\ntmap_mode(\"view\")\n\ndm2 |&gt;\n  select(ssbid, geom) |&gt;\n  filter(\"300\" &gt; 0) |&gt;\n  slice_head(n = 1000) |&gt;\ntm_shape() +\n  tm_polygons(col = \"green\")\n\n\n\ndm2 |&gt;\n  select(ssbid, geom, `300`) |&gt;\n  drop_na(`300`) |&gt;\n  slice_head(n = 1000) |&gt;\ntm_shape() +\n  tm_polygons(col = \"blue\")\n\n\n\n5.2.1 Collect bioclimatic dataset\n\nbc &lt;- dplyr::tbl(con, dbplyr::in_schema(\"helper_variables\", \"bioclimatic_regions\"))\n\n\nbc &lt;- bc |&gt;\n  mutate(geom_wkb = dbplyr::sql(\"ST_AsBinary(geom)\")) |&gt;\n  select(-geom) |&gt;\n  collect() |&gt;\n  mutate(geom = sf::st_as_sfc(geom_wkb, crs = 25833)) |&gt;\n  sf::st_as_sf(sf_column_name = \"geom\")\n\n#class(bc)\nunique(bc$BCregion)\n\nbc2 &lt;- bc |&gt; \n  group_by(BCregion) |&gt;\n  summarise()\n\nreg &lt;- unique(bc2$BCregion)\nbc2 &lt;- bc2 |&gt;\n  mutate(BCregion = case_when(\n    BCregion == reg[1] ~ 2,\n    BCregion == reg[2] ~ 1,\n    BCregion == reg[3] ~ 3,\n    BCregion == reg[4] ~ 5,\n    BCregion == reg[5] ~ 7,\n    BCregion == reg[6] ~ 6,\n    BCregion == reg[7] ~ 4,\n    BCregion == reg[8] ~ 8,\n    BCregion == reg[9] ~ 4,\n    BCregion == reg[10] ~ 9,\n    BCregion == reg[11] ~ 1,\n    BCregion == reg[12] ~ 7,\n  )) |&gt;\n  group_by(BCregion) |&gt;\n  summarise()\ntmap_mode(\"plot\")\n\npal &lt;- tmaptools::get_brewer_pal(\"Accent\", n = 9, stretch = FALSE)\nmap&lt;- bc2 |&gt;\n  tm_shape()+\n  tm_polygons(col=\"BCregion\", style=\"cat\", palette = pal, lwd=0)\n\ntmap_save(map, \"img/bcMap.png\")",
    "crumbs": [
      "Read data from database",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Read data from the database</span>"
    ]
  }
]